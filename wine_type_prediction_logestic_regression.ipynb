{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "red_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "\n",
    "red_wine['quality_label'] = red_wine['quality'].apply(lambda value: 'low'\n",
    "if value <= 5 else 'medium'\n",
    "if value <= 7 else 'high')\n",
    "\n",
    "# here we are transforming these labels into categrical data type (specific to pandas) instead of simple string\n",
    "red_wine['quality_label'] = pd.Categorical(red_wine['quality_label'],\n",
    "categories=['low', 'medium', 'high'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "white_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')\n",
    "\n",
    "\n",
    "# we are creating a new column called \"quality_label\", we define a range and associate that range with a label\n",
    "white_wine['quality_label'] = white_wine['quality'].apply(lambda value: 'low'\n",
    "if value <= 5 else 'medium'\n",
    "if value <= 7 else 'high')\n",
    "\n",
    "# here we are transforming these labels into categrical data type (specific to pandas) instead of simple string\n",
    "white_wine['quality_label'] = pd.Categorical(white_wine['quality_label'],\n",
    "categories=['low', 'medium', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality quality_label  \n",
       "0         8.8        6        medium  \n",
       "1         9.5        6        medium  \n",
       "2        10.1        6        medium  \n",
       "3         9.9        6        medium  \n",
       "4         9.9        6        medium  \n",
       "...       ...      ...           ...  \n",
       "4893     11.2        6        medium  \n",
       "4894      9.6        5           low  \n",
       "4895      9.4        6        medium  \n",
       "4896     12.8        7        medium  \n",
       "4897     11.8        6        medium  \n",
       "\n",
       "[4898 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(white_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\redi\\anaconda3\\envs\\pandas\\lib\\site-packages (1.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\redi\\anaconda3\\envs\\pandas\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\redi\\anaconda3\\envs\\pandas\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\redi\\anaconda3\\envs\\pandas\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\redi\\anaconda3\\envs\\pandas\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15579639772222578\n",
      "0.5307749515326159\n"
     ]
    }
   ],
   "source": [
    "# Epic 2 - Calculate skewness and kurtosis:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(white_wine['quality'].astype(float).skew())\n",
    "print(white_wine['pH'].astype(float).kurt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Epic 3 *- Split your data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# !!!!???????????????\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[39m=\u001b[39m df \u001b[39m# --> the features we will keep to build our model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m y \u001b[39m=\u001b[39m target \u001b[39m# --> what you're trying to predict\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Example:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Epic 3 *- Split your data\n",
    "\n",
    "# !!!!???????????????\n",
    "\n",
    "\n",
    "X = df # --> the features we will keep to build our model\n",
    "y = target # --> what you're trying to predict\n",
    "\n",
    "# Example:\n",
    "y=X.SalePrice\n",
    "X.drop(['SalePrice'],axis=1,inplace=True)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9707692307692307\n",
      "Confusion Matrix:\n",
      " [[292  22]\n",
      " [ 16 970]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       314\n",
      "           1       0.98      0.98      0.98       986\n",
      "\n",
      "    accuracy                           0.97      1300\n",
      "   macro avg       0.96      0.96      0.96      1300\n",
      "weighted avg       0.97      0.97      0.97      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 1.Loading the Data:\n",
    "import pandas as pd\n",
    "\n",
    "  # Load the datasets\n",
    "red_wine_data_set = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\")\n",
    "white_wine_data_set = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Adding the \"quality_label\" and \"wine_type\" Columns:\n",
    "\n",
    "   # Add the \"quality_label\" column\n",
    "red_wine_data_set['quality_label'] = red_wine_data_set['quality'].apply(lambda value: 'low' if value <= 5 else ('medium' if value <= 7 else 'high'))\n",
    "white_wine_data_set['quality_label'] = white_wine_data_set['quality'].apply(lambda value: 'low' if value <= 5 else ('medium' if value <= 7 else 'high'))\n",
    "\n",
    "   # Add the \"wine_type\" column\n",
    "red_wine_data_set['wine_type'] = 'red'\n",
    "white_wine_data_set['wine_type'] = 'white'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Combining the Datasets:\n",
    "combined_wine = pd.concat([white_wine_data_set, red_wine_data_set], axis=0) #  axis=0 indicates that the dataframes should be concatenated vertically\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Encoding Categorical Features:\n",
    "   # Since machine learning models like Logistic Regression expect numerical input, \n",
    "   # we encode the categorical features 'quality_label' and 'wine_type' using one-hot encoding.\n",
    "   # This converts them into binary columns.\n",
    "\n",
    "\n",
    " # Encode \"quality_label\" using .cat.codes\n",
    " # low: 1 / medium: 2 / high: 0\n",
    "combined_wine['quality_label_encoded'] = combined_wine['quality_label'].astype('category').cat.codes\n",
    "\n",
    " # Encode \"wine_type\" using .cat.codes\n",
    " # White: 1 / Red: 0\n",
    "combined_wine['wine_type_encoded'] = combined_wine['wine_type'].astype('category').cat.codes\n",
    "\n",
    "\n",
    " # Identify categorical columns\n",
    "categorical_columns_to_drop = ['wine_type', 'quality_label']\n",
    "\n",
    "\n",
    " # Drop categorical columns\n",
    "combined_wine = combined_wine.drop(categorical_columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Splitting the Data for Machine Learning:\n",
    "  # We split the data into features (X) and the target variable (y), which is 'wine_type_white' indicating whether the wine is white (1) or not (0).\n",
    "  # The dataset is divided into training (80%) and testing (20%) sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_wine.drop('wine_type_encoded', axis=1)  # Features\n",
    "y = combined_wine['wine_type_encoded']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "'''\n",
    " # To exclude many columns when setting your training features\n",
    "  # List of columns to exclude\n",
    "columns_to_exclude = ['volatile acidity', 'citric acid', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'fixed acidity', 'residual sugar']\n",
    "\n",
    "  # Select columns for training by excluding the ones in the list\n",
    "X = combined_wine.drop(columns=columns_to_exclude)\n",
    "\n",
    "  # Your target variable\n",
    "y = combined_wine['wine_type_encoded']\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Applying Logistic Regression:\n",
    "  # We import the Logistic Regression model from scikit-learn and create an instance of the model.\n",
    "  # Then, we fit the model to the training data.\n",
    "  # Fitting the model means training the model on training data using the .fit method provided in sklearn.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 7. Evaluating the Model:\n",
    "  # To assess the model's performance, we use it to make predictions on the test data.\n",
    "  # We calculate accuracy, confusion matrix, and a classification report to provide more detailed metrics.\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n",
    "# classification_report and confusion_matrix are two commonly used tools in machine learning \n",
    "# for evaluating the performance of classification models, \n",
    "# especially in tasks where you are predicting categorical outcomes (i.e., classes or labels). \n",
    "# They provide valuable insights into how well your model is performing and where it might be making errors. \n",
    "# These tools are typically used in supervised learning tasks, such as binary or multiclass classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Perform Feature scaling</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# data normalisation with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transform testing data\n",
    "X_test_norm = norm.transform(X_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Sprint 4 - Epic 3: model improvement</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores for Each Fold: [0.96461538 0.96846154 0.99076212 0.97382602 0.98229407]\n",
      "Mean Accuracy: 0.9759918280333985\n",
      "Standard Deviation of Accuracy: 0.009473113509278522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# to evaluate machine learning models through cross-validation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming you've loaded and preprocessed your dataset, X and y should be defined here.\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy as the scoring metric\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# View the results\n",
    "print(\"Accuracy Scores for Each Fold:\", scores)\n",
    "print(\"Mean Accuracy:\", np.mean(scores))\n",
    "print(\"Standard Deviation of Accuracy:\", np.std(scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultimately, the decision to include \"wine_type_encoded\" or \"wine_type\" in your logistic regression model depends on\n",
    "# depends on whether it adds valuable information to predict the \"quality_label\" and whether it aligns with the objectives of your analysis.\n",
    "# You can experiment with both scenarios and evaluate which model performs better and makes more sense in the context of your wine quality prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Predicting Wine Quality</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9715384615384616\n",
      "Confusion Matrix:\n",
      " [[  9   0  23]\n",
      " [  0 464   4]\n",
      " [  6   4 790]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.28      0.38        32\n",
      "           1       0.99      0.99      0.99       468\n",
      "           2       0.97      0.99      0.98       800\n",
      "\n",
      "    accuracy                           0.97      1300\n",
      "   macro avg       0.85      0.75      0.78      1300\n",
      "weighted avg       0.97      0.97      0.97      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "  # Load the datasets\n",
    "red_wine_data_set = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\")\n",
    "white_wine_data_set = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Adding the \"quality_label\" and \"wine_type\" Columns:\n",
    "\n",
    "   # Add the \"quality_label\" column\n",
    "red_wine_data_set['quality_label'] = red_wine_data_set['quality'].apply(lambda value: 'low' if value <= 5 else ('medium' if value <= 7 else 'high'))\n",
    "white_wine_data_set['quality_label'] = white_wine_data_set['quality'].apply(lambda value: 'low' if value <= 5 else ('medium' if value <= 7 else 'high'))\n",
    "\n",
    "   # Add the \"wine_type\" column\n",
    "red_wine_data_set['wine_type'] = 'red'\n",
    "white_wine_data_set['wine_type'] = 'white'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Combining the Datasets:\n",
    "combined_wine = pd.concat([white_wine_data_set, red_wine_data_set], axis=0) #  axis=0 indicates that the dataframes should be concatenated vertically\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Encoding Categorical Features:\n",
    "   # Since machine learning models like Logistic Regression expect numerical input, \n",
    "   # we encode the categorical features 'quality_label' and 'wine_type' using one-hot encoding.\n",
    "   # This converts them into binary columns.\n",
    "\n",
    "\n",
    " # Encode \"quality_label\" using .cat.codes\n",
    " # low: 1 / medium: 2 / high: 0\n",
    "combined_wine['quality_label_encoded'] = combined_wine['quality_label'].astype('category').cat.codes\n",
    "\n",
    " # Encode \"wine_type\" using .cat.codes\n",
    " # White: 1 / Red: 0\n",
    "combined_wine['wine_type_encoded'] = combined_wine['wine_type'].astype('category').cat.codes\n",
    "\n",
    "\n",
    " # Identify categorical columns\n",
    "categorical_columns_to_drop = ['wine_type', 'quality_label']\n",
    "\n",
    "\n",
    " # Drop categorical columns\n",
    "combined_wine = combined_wine.drop(categorical_columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Splitting the Data for Machine Learning:\n",
    "  # We split the data into features (X) and the target variable (y), which is 'wine_type_white' indicating whether the wine is white (1) or not (0).\n",
    "  # The dataset is divided into training (80%) and testing (20%) sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xq = combined_wine.drop('quality_label_encoded', axis=1)  # Features\n",
    "yq = combined_wine['quality_label_encoded']  # Target variable\n",
    "\n",
    "Xq_train, Xq_test, yq_train, yq_test = train_test_split(Xq, yq, test_size=0.2, random_state=42)\n",
    "\n",
    "'''\n",
    " # To exclude many columns when setting your training features\n",
    "  # List of columns to exclude\n",
    "columns_to_exclude = ['volatile acidity', 'citric acid', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'fixed acidity', 'residual sugar']\n",
    "\n",
    "  # Select columns for training by excluding the ones in the list\n",
    "X = combined_wine.drop(columns=columns_to_exclude)\n",
    "\n",
    "  # Your target variable\n",
    "y = combined_wine['wine_type_encoded']\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Applying Logistic Regression:\n",
    "  # We import the Logistic Regression model from scikit-learn and create an instance of the model.\n",
    "  # Then, we fit the model to the training data.\n",
    "  # Fitting the model means training the model on training data using the .fit method provided in sklearn.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xq_train, yq_train)\n",
    "\n",
    "\n",
    "# 7. Evaluating the Model:\n",
    "  # To assess the model's performance, we use it to make predictions on the test data.\n",
    "  # We calculate accuracy, confusion matrix, and a classification report to provide more detailed metrics.\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "yq_pred = model.predict(Xq_test)\n",
    "\n",
    "accuracy = accuracy_score(yq_test, yq_pred)\n",
    "confusion = confusion_matrix(yq_test, yq_pred)\n",
    "report = classification_report(yq_test, yq_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n",
    "# classification_report and confusion_matrix are two commonly used tools in machine learning \n",
    "# for evaluating the performance of classification models, \n",
    "# especially in tasks where you are predicting categorical outcomes (i.e., classes or labels). \n",
    "# They provide valuable insights into how well your model is performing and where it might be making errors. \n",
    "# These tools are typically used in supervised learning tasks, such as binary or multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores for Each Fold: [0.95923077 0.94307692 0.93148576 0.90839107 0.926097  ]\n",
      "Mean Accuracy: 0.9336563036655414\n",
      "Standard Deviation of Accuracy: 0.016988159185775238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ReDI\\anaconda3\\envs\\Pandas\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# to evaluate machine learning models through cross-validation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming you've loaded and preprocessed your dataset, X and y should be defined here.\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy as the scoring metric\n",
    "scores = cross_val_score(model, Xq, yq, cv=5, scoring='accuracy')\n",
    "\n",
    "# View the results\n",
    "print(\"Accuracy Scores for Each Fold:\", scores)\n",
    "print(\"Mean Accuracy:\", np.mean(scores))\n",
    "print(\"Standard Deviation of Accuracy:\", np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
